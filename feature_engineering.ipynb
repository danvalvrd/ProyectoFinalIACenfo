{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33f4493a",
   "metadata": {},
   "source": [
    "# Ingeniería de Características para Segmentación de la Calidad del Agua Subterránea\n",
    "\n",
    "### Estudiantes\n",
    "Amanda Alpízar Araya\n",
    "Alonso Arias Mora\n",
    "Danny Valverde Agüero\n",
    "\n",
    "### Introducción y Objetivos\n",
    "\n",
    "Esta sección detalla el proceso de ingeniería de características para el conjunto de datos sobre la calidad del agua de 1,068 sitios de monitoreo de aguas subterráneas en México. El objetivo es transformar los datos crudos en un formato optimizado para algoritmos de clustering no supervisado, maximizando la información relevante mientras se mitigan problemas como el sesgo, la alta dimensionalidad y la multicolinealidad.\n",
    "\n",
    "El proceso de ingeniería de características se basa en hallazgos clave del Análisis Exploratorio de Datos (EDA), que reveló:\n",
    "1. **Distribuciones altamente sesgadas** en variables fisicoquímicas que requieren transformaciones no lineales\n",
    "2. **Alta correlación entre variables de salinidad** (CONDUCT_mS/cm y SDT_M_mg/L)\n",
    "3. **Patrones geográficos significativos** que sugieren relaciones espaciales importantes\n",
    "4. **Variables categóricas de alta cardinalidad** (MUNICIPIO: 452, ACUIFERO: 273)\n",
    "5. **Indicadores multi-parámetro** de calidad del agua que podrían beneficiarse de características compuestas\n",
    "\n",
    "La metodología implementada sigue un enfoque sistemático que incluye:\n",
    "1. **Análisis de correlación** para la selección de características no redundantes\n",
    "2. **Generación de características compuestas** basadas en conocimiento de dominio\n",
    "3. **Transformaciones logarítmicas** para mitigar el sesgo\n",
    "4. **Estandarización** para uniformizar escalas\n",
    "5. **Reducción de dimensionalidad** para capturar patrones subyacentes\n",
    "\n",
    "El resultado final será un conjunto de datos optimizado para algoritmos de clustering, que facilitará la identificación de perfiles significativos de calidad del agua."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c755f9a0",
   "metadata": {},
   "source": [
    "### Sección 1: Configuración del Entorno y Carga de Datos\n",
    "\n",
    "#### Resumen Ejecutivo de la Sección\n",
    "\n",
    "El objetivo de esta sección inicial es configurar el entorno de trabajo necesario para la ingeniería de características y cargar el conjunto de datos limpio producido durante la fase de EDA. Importaremos las librerías esenciales para análisis de datos, visualización y transformaciones, y configuraremos el estilo visual para mantener consistencia con el análisis exploratorio.\n",
    "\n",
    "La configuración incluye la supresión de advertencias para mantener una salida limpia y la definición de parámetros estándar para visualizaciones. Luego, cargaremos el dataset previamente limpiado (`df_cleaned.csv`), que contiene 1,068 registros con mediciones de calidad del agua y variables categóricas ya procesadas.\n",
    "\n",
    "Esta sección establece la base para todo el proceso de ingeniería de características, asegurando que partimos de datos confiables y con un entorno de trabajo optimizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecfc999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración del Entorno y Carga de Datos Limpios ---\n",
    "\n",
    "# Importar las librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Ignorar advertencias para una salida limpia\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurar el estilo y tamaño de las gráficas\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# --- 1. Cargar el conjunto de datos limpio ---\n",
    "# Este archivo es el resultado del notebook EDA.ipynb\n",
    "cleaned_data_path = 'datos/df_cleaned.csv'\n",
    "try:\n",
    "    # Asumimos que el df_cleaned fue guardado con su índice, por lo que lo usamos como index_col\n",
    "    df_cleaned = pd.read_csv(cleaned_data_path, index_col=0)\n",
    "    print(f\"El conjunto de datos limpio se cargó exitosamente desde: '{cleaned_data_path}'.\")\n",
    "    print(f\"Dimensiones del dataset: {df_cleaned.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró el archivo en la ruta: '{cleaned_data_path}'\")\n",
    "    print(\"Por favor, ejecute primero el notebook EDA.ipynb para generar este archivo.\")\n",
    "    df_cleaned = None\n",
    "\n",
    "# --- 2. Mostrar información inicial del DataFrame ---\n",
    "if df_cleaned is not None:\n",
    "    print(\"\\n--- Primeras 5 filas del conjunto de datos limpio ---\")\n",
    "    display(df_cleaned.head())\n",
    "    print(\"\\n--- Tipos de datos de las columnas ---\")\n",
    "    df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8120ebc7",
   "metadata": {},
   "source": [
    "### Sección 2: Análisis de Correlación y Selección de Características\n",
    "\n",
    "#### Resumen Ejecutivo de la Sección\n",
    "\n",
    "Esta sección aborda uno de los principales desafíos identificados en el EDA: la redundancia de información entre variables altamente correlacionadas. El objetivo es identificar y eliminar sistemáticamente esta redundancia para:\n",
    "1. Reducir la dimensionalidad innecesaria\n",
    "2. Evitar que el algoritmo de clustering asigne peso excesivo a información duplicada\n",
    "3. Mejorar la eficiencia computacional sin pérdida significativa de información\n",
    "\n",
    "La metodología implementada consiste en:\n",
    "1. Seleccionar inicialmente todas las variables de medición para su análisis\n",
    "2. Calcular la matriz de correlación absoluta entre estas variables\n",
    "3. Identificar pares de variables con correlación superior a un umbral definido (r ≥ 0.8)\n",
    "4. Aplicar un algoritmo para determinar qué variable de cada par debe conservarse\n",
    "\n",
    "El criterio de selección dentro de cada par altamente correlacionado prioriza la variable con menos valores faltantes originalmente, lo que generalmente indica mayor confiabilidad en la medición.\n",
    "\n",
    "Al final de esta sección, obtendremos un conjunto optimizado de variables para las siguientes etapas del proceso de ingeniería de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a51f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sección 2: Análisis de Correlación y Selección de Características ---\n",
    "\n",
    "if df_cleaned is not None:\n",
    "    print(\"--- Análisis de Correlación para Selección de Características ---\\n\")\n",
    "    \n",
    "    # Seleccionar todas las columnas de mediciones físico-químicas para análisis\n",
    "    measurement_cols = [\n",
    "        'ALC_mg/L', 'CONDUCT_mS/cm', 'SDT_M_mg/L', 'FLUORUROS_mg/L', \n",
    "        'DUR_mg/L', 'COLI_FEC_NMP/100_mL', 'N_NO3_mg/L', 'AS_TOT_mg/L',\n",
    "        'CD_TOT_mg/L', 'CR_TOT_mg/L', 'HG_TOT_mg/L', 'PB_TOT_mg/L',\n",
    "        'MN_TOT_mg/L', 'FE_TOT_mg/L'\n",
    "    ]\n",
    "    \n",
    "    geo_cols = ['LONGITUD', 'LATITUD']\n",
    "    \n",
    "    print(f\"Seleccionando variables de medición para análisis de correlación...\")\n",
    "    print(f\"Se analizarán {len(measurement_cols)} variables de medición.\\n\")\n",
    "    \n",
    "    # Calcular matriz de correlación\n",
    "    print(\"Calculando matriz de correlación...\")\n",
    "    corr_matrix = df_cleaned[measurement_cols].corr().abs()\n",
    "    \n",
    "    # Visualizar la matriz de correlación\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    \n",
    "    sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=1, vmin=0,\n",
    "                center=0.5, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    plt.title('Matriz de Correlación Absoluta entre Variables de Medición', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identificar pares de variables altamente correlacionadas\n",
    "    threshold = 0.8\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if corr_matrix.iloc[i, j] >= threshold:\n",
    "                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "    \n",
    "    # Mostrar pares altamente correlacionados\n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\nSe encontraron {len(high_corr_pairs)} pares de variables con alta correlación (r ≥ {threshold}):\")\n",
    "        for col1, col2, corr in high_corr_pairs:\n",
    "            print(f\"  - {col1} y {col2}: r = {corr:.4f}\")\n",
    "        \n",
    "        # Algoritmo para seleccionar qué variables eliminar\n",
    "        features_to_drop = set()\n",
    "        for col1, col2, _ in high_corr_pairs:\n",
    "            # Determinar cuál variable tenía originalmente más valores faltantes\n",
    "            missing_col1 = df_cleaned[col1].isna().sum()\n",
    "            missing_col2 = df_cleaned[col2].isna().sum()\n",
    "            \n",
    "            # Elegir la variable con más valores faltantes para eliminar\n",
    "            if missing_col1 > missing_col2:\n",
    "                features_to_drop.add(col1)\n",
    "            else:\n",
    "                features_to_drop.add(col2)\n",
    "        \n",
    "        print(f\"\\nSe eliminarán {len(features_to_drop)} variables debido a alta correlación:\")\n",
    "        for feature in features_to_drop:\n",
    "            print(f\"  - {feature}\")\n",
    "        \n",
    "        # Actualizar la lista de variables seleccionadas\n",
    "        selected_measurements = [col for col in measurement_cols if col not in features_to_drop]\n",
    "        print(f\"\\nSe conservarán {len(selected_measurements)} variables de medición:\")\n",
    "        for feature in selected_measurements:\n",
    "            print(f\"  - {feature}\")\n",
    "    else:\n",
    "        print(f\"\\nNo se encontraron pares de variables con correlación mayor o igual a {threshold}\")\n",
    "        selected_measurements = measurement_cols\n",
    "        \n",
    "    # Crear un DataFrame con las variables seleccionadas\n",
    "    df_selected = df_cleaned[geo_cols + selected_measurements].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5c237",
   "metadata": {},
   "source": [
    "### Sección 3: Generación de Características Compuestas\n",
    "\n",
    "#### Resumen Ejecutivo de la Sección\n",
    "\n",
    "En esta sección aplicamos el proceso de ingeniería de características para crear nuevas variables que capturen información más compleja y relevante que las mediciones individuales originales. El objetivo es desarrollar características compuestas que:\n",
    "\n",
    "1. Integren múltiples dimensiones de la calidad del agua en métricas unificadas\n",
    "2. Capturen relaciones entre contaminantes que son significativas para la interpretación\n",
    "3. Reflejen patrones geográficos y contextuales que podrían influir en la agrupación\n",
    "\n",
    "Basándonos en el conocimiento del dominio extraído del EDA, generamos tres tipos de características compuestas:\n",
    "\n",
    "1. **Índice de Calidad del Agua**: Un indicador ponderado que integra los principales contaminantes según su importancia relativa para la salud y el uso del agua.\n",
    "\n",
    "2. **Indicadores Regionales**: Características que capturan la proximidad geográfica entre puntos de monitoreo, lo que permite identificar patrones espaciales.\n",
    "\n",
    "3. **Ratios de Balance Mineral**: Relaciones entre diferentes parámetros que son relevantes para evaluar el equilibrio químico del agua.\n",
    "\n",
    "Estas características compuestas proporcionarán información adicional que las mediciones individuales no pueden capturar por sí solas, mejorando así la capacidad del algoritmo de clustering para identificar patrones significativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b0f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sección 3: Generación de Características Compuestas ---\n",
    "\n",
    "if df_cleaned is not None:\n",
    "    print(\"--- Generación de Características Compuestas ---\\n\")\n",
    "    \n",
    "    # Crear copia para generación de características\n",
    "    df_features = df_selected.copy()\n",
    "    \n",
    "    # 1. Crear índice de calidad del agua ponderado\n",
    "    print(\"1. Creando índice de calidad del agua ponderado...\")\n",
    "    \n",
    "    # Definir pesos basados en la importancia para la salud y calidad del agua\n",
    "    # Estos pesos se basan en el análisis del EDA y la frecuencia de contaminantes\n",
    "    weights = {\n",
    "        'AS_TOT_mg/L': 0.20,      # Alto impacto en salud humana\n",
    "        'FLUORUROS_mg/L': 0.15,   # Impacto significativo en salud\n",
    "        'N_NO3_mg/L': 0.15,       # Importante para calidad de agua potable\n",
    "        'CONDUCT_mS/cm': 0.10,    # Indicador de mineralización\n",
    "        'FE_TOT_mg/L': 0.10,      # Contaminante común\n",
    "        'MN_TOT_mg/L': 0.10,      # Contaminante común\n",
    "        'COLI_FEC_NMP/100_mL': 0.20  # Indicador crítico de contaminación biológica\n",
    "    }\n",
    "    \n",
    "    # Normalizar cada parámetro al rango [0,1] usando percentiles para reducir influencia de outliers\n",
    "    normalized_params = {}\n",
    "    for param, weight in weights.items():\n",
    "        if param in df_features.columns:\n",
    "            q_min, q_max = df_features[param].quantile([0.05, 0.95])\n",
    "            normalized = (df_features[param] - q_min) / (q_max - q_min)\n",
    "            normalized = normalized.clip(0, 1)  # Limitar valores a [0,1]\n",
    "            normalized_params[param] = normalized\n",
    "    \n",
    "    # Calcular el índice ponderado\n",
    "    df_features['indice_calidad_agua'] = sum(\n",
    "        normalized_params[param] * weight \n",
    "        for param, weight in weights.items() \n",
    "        if param in normalized_params\n",
    "    )\n",
    "    \n",
    "    # 2. Crear indicadores regionales basados en proximidad geográfica\n",
    "    print(\"2. Creando indicadores regionales basados en proximidad geográfica...\")\n",
    "    \n",
    "    # Calcular matriz de distancias entre todos los puntos\n",
    "    coords = df_features[['LONGITUD', 'LATITUD']].values\n",
    "    distances = pdist(coords)  # Distancia euclidiana entre todos los pares de puntos\n",
    "    dist_matrix = squareform(distances)  # Convertir a matriz\n",
    "    \n",
    "    # Calcular distancia al vecino más cercano\n",
    "    df_features['distancia_vecino_cercano'] = [\n",
    "        sorted(dist_matrix[i])[1] for i in range(len(dist_matrix))\n",
    "    ]\n",
    "    \n",
    "    # 3. Calcular ratios de balance mineral\n",
    "    print(\"3. Calculando ratios de balance mineral...\")\n",
    "    \n",
    "    # Ratio de conductividad a dureza (proxy de SAR - Sodium Adsorption Ratio)\n",
    "    if 'CONDUCT_mS/cm' in df_features.columns and 'DUR_mg/L' in df_features.columns:\n",
    "        df_features['ratio_conduct_dureza'] = df_features['CONDUCT_mS/cm'] / (df_features['DUR_mg/L'] + 0.001)\n",
    "    \n",
    "    # Ratio de alcalinidad a dureza (equilibrio carbonato-calcio)\n",
    "    if 'ALC_mg/L' in df_features.columns and 'DUR_mg/L' in df_features.columns:\n",
    "        df_features['ratio_alc_dureza'] = df_features['ALC_mg/L'] / (df_features['DUR_mg/L'] + 0.001)\n",
    "    \n",
    "    # Mostrar resumen de las nuevas características generadas\n",
    "    new_features = [col for col in df_features.columns if col not in df_selected.columns]\n",
    "    \n",
    "    print(f\"\\nSe han generado {len(new_features)} nuevas características compuestas:\")\n",
    "    for feature in new_features:\n",
    "        print(f\"  - {feature}\")\n",
    "    \n",
    "    # Mostrar estadísticas descriptivas de las nuevas características\n",
    "    print(\"\\nEstadísticas descriptivas de las nuevas características:\")\n",
    "    display(df_features[new_features].describe())\n",
    "    \n",
    "    # Visualizar la distribución del índice de calidad del agua\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df_features['indice_calidad_agua'], kde=True)\n",
    "    plt.title('Distribución del Índice de Calidad del Agua', fontsize=14)\n",
    "    plt.xlabel('Índice (mayor valor indica más contaminación)', fontsize=12)\n",
    "    plt.ylabel('Frecuencia', fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abeb77",
   "metadata": {},
   "source": [
    "### Sección 4: Transformación y Normalización de Datos\n",
    "\n",
    "#### Resumen Ejecutivo de la Sección\n",
    "\n",
    "En esta sección abordamos uno de los hallazgos más importantes del EDA: la distribución altamente sesgada de la mayoría de las variables de medición. Estas distribuciones no normales pueden afectar negativamente el rendimiento de los algoritmos de clustering, especialmente aquellos basados en distancias euclidianas.\n",
    "\n",
    "El proceso de transformación consta de dos etapas principales:\n",
    "\n",
    "1. **Transformación Logarítmica**: Aplicamos la transformación logarítmica (log1p) a todas las variables de medición excepto las coordenadas geográficas. Esta transformación reduce el sesgo positivo, comprime el rango de valores extremos y aproxima las distribuciones a la normalidad. Utilizamos log1p (logaritmo de x+1) en lugar del logaritmo simple para manejar adecuadamente los valores cercanos o iguales a cero.\n",
    "\n",
    "2. **Estandarización**: Aplicamos la estandarización (z-score) a todas las variables, incluidas las geográficas, para que todas tengan media cero y desviación estándar uno. Esto garantiza que todas las variables contribuyan por igual al análisis de clustering independientemente de sus escalas originales.\n",
    "\n",
    "Estas transformaciones son esenciales para la efectividad del clustering, ya que los algoritmos basados en distancia (como K-means) son sensibles a las diferencias en escalas y a las distribuciones sesgadas. Al normalizar y estandarizar los datos, aseguramos que el algoritmo capture correctamente los patrones subyacentes sin estar indebidamente influenciado por valores extremos o diferencias de escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "032eb969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sección 4: Transformación y Normalización de Datos ---\n",
    "\n",
    "if df_cleaned is not None:\n",
    "    print(\"--- Transformación y Normalización de Datos ---\\n\")\n",
    "    \n",
    "    # Crear copia para transformaciones\n",
    "    df_transform = df_features.copy()\n",
    "    \n",
    "    # 1. Transformación logarítmica para variables con distribución sesgada\n",
    "    # Excluir coordenadas geográficas y las características ya generadas\n",
    "    skip_cols = ['LONGITUD', 'LATITUD', 'indice_calidad_agua', 'distancia_vecino_cercano', \n",
    "                 'ratio_conduct_dureza', 'ratio_alc_dureza']\n",
    "    log_transform_cols = [col for col in df_transform.columns if col not in skip_cols]\n",
    "    \n",
    "    print(\"1. Aplicando transformación logarítmica...\")\n",
    "    print(f\"Se transformarán {len(log_transform_cols)} variables:\")\n",
    "    for col in log_transform_cols[:5]:\n",
    "        print(f\"  - {col}\")\n",
    "    if len(log_transform_cols) > 5:\n",
    "        print(f\"  ... y {len(log_transform_cols)-5} más\")\n",
    "    \n",
    "    # Visualizar distribuciones antes de la transformación (muestra de 4 variables)\n",
    "    sample_cols = [col for col in log_transform_cols if col in selected_measurements][:4]\n",
    "    if sample_cols:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i, col in enumerate(sample_cols):\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            sns.histplot(df_transform[col], kde=True)\n",
    "            plt.title(f'Distribución de {col} (Original)', fontsize=12)\n",
    "            plt.xlabel(col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Aplicar transformación logarítmica\n",
    "    for col in log_transform_cols:\n",
    "        df_transform[col] = np.log1p(df_transform[col])\n",
    "    \n",
    "    # Visualizar distribuciones después de la transformación\n",
    "    if sample_cols:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i, col in enumerate(sample_cols):\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            sns.histplot(df_transform[col], kde=True)\n",
    "            plt.title(f'Distribución de {col} (Log-transformada)', fontsize=12)\n",
    "            plt.xlabel(f'log(1+{col})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. Estandarización (z-score) de todas las variables\n",
    "    print(\"\\n2. Aplicando estandarización (z-score) a todas las variables...\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df_transform)\n",
    "    \n",
    "    # Crear DataFrame con datos escalados\n",
    "    df_scaled = pd.DataFrame(\n",
    "        scaled_data,\n",
    "        index=df_transform.index,\n",
    "        columns=df_transform.columns\n",
    "    )\n",
    "    \n",
    "    # Mostrar primeras filas de los datos transformados y escalados\n",
    "    print(\"\\nPrimeras filas de los datos transformados y escalados:\")\n",
    "    display(df_scaled.head())\n",
    "    \n",
    "    # Verificar que la estandarización funcionó correctamente\n",
    "    mean_std = pd.DataFrame({\n",
    "        'Media': df_scaled.mean(),\n",
    "        'Desviación Estándar': df_scaled.std()\n",
    "    })\n",
    "    \n",
    "    print(\"\\nVerificación de la estandarización (valores cercanos a 0 y 1 respectivamente):\")\n",
    "    display(mean_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ace75",
   "metadata": {},
   "source": [
    "### Sección 5: Reducción de Dimensionalidad con PCA\n",
    "\n",
    "#### Resumen Ejecutivo de la Sección\n",
    "\n",
    "Esta sección implementa la reducción de dimensionalidad mediante Análisis de Componentes Principales (PCA), una técnica esencial para preparar datos de alta dimensionalidad para algoritmos de clustering. La aplicación de PCA tiene varios objetivos fundamentales:\n",
    "\n",
    "1. **Reducir la complejidad del modelo** manteniendo la máxima varianza posible\n",
    "2. **Mitigar el problema de la maldición de la dimensionalidad**, que afecta negativamente a los algoritmos basados en distancias\n",
    "3. **Eliminar la multicolinealidad residual** que pudiera persistir incluso después del análisis de correlación\n",
    "4. **Facilitar la visualización** de los datos en espacios de menor dimensión\n",
    "\n",
    "El enfoque implementado consiste en:\n",
    "1. Aplicar PCA configurado para capturar al menos el 90% de la varianza total\n",
    "2. Analizar la cantidad de componentes necesarias y su contribución individual a la varianza explicada\n",
    "3. Examinar la composición de cada componente principal para comprender qué variables originales influyen más en cada una\n",
    "4. Conservar tanto la matriz de datos transformada como la información de importancia de variables\n",
    "\n",
    "Este paso proporciona un conjunto de datos optimizado para el clustering, donde cada componente principal representa una combinación lineal de las variables originales, capturando diferentes aspectos de la variabilidad de los datos de calidad del agua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a50676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sección 5: Reducción de Dimensionalidad con PCA ---\n",
    "\n",
    "if df_cleaned is not None:\n",
    "    print(\"--- Reducción de Dimensionalidad con PCA ---\\n\")\n",
    "    \n",
    "    # Aplicar PCA configurado para capturar al menos el 90% de la varianza\n",
    "    print(\"Aplicando PCA para capturar al menos el 90% de la varianza...\")\n",
    "    pca = PCA(n_components=0.90)\n",
    "    pca_result = pca.fit_transform(df_scaled)\n",
    "    \n",
    "    # Crear DataFrame con los componentes principales\n",
    "    pca_cols = [f'PC{i+1}' for i in range(pca_result.shape[1])]\n",
    "    df_pca = pd.DataFrame(\n",
    "        pca_result,\n",
    "        index=df_scaled.index,\n",
    "        columns=pca_cols\n",
    "    )\n",
    "    \n",
    "    # Analizar la varianza explicada\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    print(f\"\\nSe identificaron {len(pca_cols)} componentes principales que explican el {cumulative_variance[-1]*100:.2f}% de la varianza total\")\n",
    "    \n",
    "    # Visualizar la varianza explicada por cada componente\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(pca_cols, explained_variance * 100)\n",
    "    plt.plot(pca_cols, cumulative_variance * 100, 'ro-')\n",
    "    plt.axhline(y=90, color='r', linestyle='--')\n",
    "    plt.title('Varianza Explicada por Componente Principal', fontsize=14)\n",
    "    plt.xlabel('Componente Principal', fontsize=12)\n",
    "    plt.ylabel('Porcentaje de Varianza Explicada', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar la contribución de cada componente principal\n",
    "    print(\"\\nContribución de cada componente principal a la varianza total:\")\n",
    "    for i, (var, cum_var) in enumerate(zip(explained_variance, cumulative_variance)):\n",
    "        print(f\"  - PC{i+1}: {var*100:.2f}% varianza ({cum_var*100:.2f}% acumulada)\")\n",
    "    \n",
    "    # Analizar la composición de los primeros componentes principales\n",
    "    components_df = pd.DataFrame(\n",
    "        pca.components_,\n",
    "        columns=df_scaled.columns,\n",
    "        index=pca_cols\n",
    "    )\n",
    "    \n",
    "    # Mostrar las 3 variables más influyentes para cada uno de los primeros 3 componentes\n",
    "    print(\"\\nVariables más influyentes en los primeros componentes principales:\")\n",
    "    for i in range(min(3, len(pca_cols))):\n",
    "        comp = components_df.iloc[i].abs()\n",
    "        top_vars = comp.nlargest(3)\n",
    "        print(f\"\\n  PC{i+1} ({explained_variance[i]*100:.2f}% varianza):\")\n",
    "        for var, weight in top_vars.items():\n",
    "            print(f\"    - {var}: {weight:.4f}\")\n",
    "    \n",
    "    # Visualizar mapa de calor de la matriz de componentes\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(components_df.iloc[:min(8, len(pca_cols))], cmap='coolwarm', center=0)\n",
    "    plt.title('Coeficientes de los Componentes Principales', fontsize=14)\n",
    "    plt.ylabel('Componente Principal', fontsize=12)\n",
    "    plt.xlabel('Variable Original', fontsize=12)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar las primeras filas de los datos transformados con PCA\n",
    "    print(\"\\nPrimeras filas de los datos transformados con PCA:\")\n",
    "    display(df_pca.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfea5b21",
   "metadata": {},
   "source": [
    "### Sección 6: Preparación Final y Exportación de Datos\n",
    "\n",
    "#### Resumen Ejecutivo de la Sección\n",
    "\n",
    "Esta sección final completa el proceso de ingeniería de características preparando y exportando los conjuntos de datos optimizados para el análisis de clustering. El objetivo es proporcionar múltiples representaciones de los datos, cada una con diferentes ventajas según la técnica de clustering y los objetivos de análisis específicos.\n",
    "\n",
    "Preparamos tres conjuntos de datos complementarios:\n",
    "\n",
    "1. **Conjunto con todas las características escaladas**: Incluye todas las variables originales seleccionadas y las características compuestas, después de la transformación logarítmica y estandarización. Este conjunto conserva la interpretabilidad directa de las variables.\n",
    "\n",
    "2. **Conjunto con componentes principales**: Contiene los componentes principales que capturan al menos el 90% de la varianza. Este conjunto es óptimo para algoritmos sensibles a la dimensionalidad y la multicolinealidad.\n",
    "\n",
    "3. **Conjunto híbrido**: Combina coordenadas geográficas con componentes principales, preservando la información espacial explícita mientras se beneficia de la reducción de dimensionalidad para las variables fisicoquímicas.\n",
    "\n",
    "Adicionalmente, exportamos la matriz de importancia de variables en los componentes principales, lo que facilitará la interpretación posterior de los resultados del clustering.\n",
    "\n",
    "Esta estrategia flexible permite adaptar el enfoque de clustering según los resultados preliminares y proporciona múltiples perspectivas para la segmentación de la calidad del agua subterránea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f18b10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sección 6: Preparación Final y Exportación de Datos ---\n",
    "\n",
    "if df_cleaned is not None:\n",
    "    print(\"--- Preparación Final y Exportación de Datos ---\\n\")\n",
    "    \n",
    "    # 1. Preparar conjunto de datos con todas las características escaladas\n",
    "    final_df_all_features = df_scaled.copy()\n",
    "    \n",
    "    # 2. Conjunto de datos con componentes principales\n",
    "    final_df_pca = df_pca.copy()\n",
    "    \n",
    "    # 3. Conjunto híbrido: coordenadas geográficas + componentes principales\n",
    "    # Extraer coordenadas del conjunto escalado\n",
    "    geo_scaled = df_scaled[['LONGITUD', 'LATITUD']]\n",
    "    \n",
    "    # Combinar con componentes principales\n",
    "    final_df_hybrid = pd.concat([geo_scaled, df_pca], axis=1)\n",
    "    \n",
    "    # Resumen de los conjuntos de datos preparados\n",
    "    print(\"Conjuntos de datos preparados para clustering:\")\n",
    "    print(f\"1. Conjunto con todas las características escaladas: {final_df_all_features.shape} (filas x columnas)\")\n",
    "    print(f\"2. Conjunto con componentes principales: {final_df_pca.shape} (filas x columnas)\")\n",
    "    print(f\"3. Conjunto híbrido (geo + PCA): {final_df_hybrid.shape} (filas x columnas)\")\n",
    "    \n",
    "    # Exportar conjuntos de datos a archivos CSV\n",
    "    try:\n",
    "        final_df_all_features.to_csv('datos/df_features_scaled.csv')\n",
    "        final_df_pca.to_csv('datos/df_pca.csv')\n",
    "        final_df_hybrid.to_csv('datos/df_hybrid.csv')\n",
    "        \n",
    "        # Exportar matriz de importancia de variables en componentes\n",
    "        components_df.to_csv('datos/pca_components.csv')\n",
    "        \n",
    "        print(\"\\nConjuntos de datos exportados exitosamente a la carpeta 'datos/'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError al exportar los datos: {str(e)}\")\n",
    "    \n",
    "    # Mostrar ejemplos de los tres conjuntos de datos\n",
    "    print(\"\\nEjemplo del conjunto con todas las características (primeras 3 filas):\")\n",
    "    display(final_df_all_features.head(3))\n",
    "    \n",
    "    print(\"\\nEjemplo del conjunto con componentes principales (primeras 3 filas):\")\n",
    "    display(final_df_pca.head(3))\n",
    "    \n",
    "    print(\"\\nEjemplo del conjunto híbrido (primeras 3 filas):\")\n",
    "    display(final_df_hybrid.head(3))\n",
    "    \n",
    "    print(\"\\n--- Proceso de Ingeniería de Características Completado ---\")\n",
    "    print(\"Los datos están ahora optimizados para algoritmos de clustering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e87b30",
   "metadata": {},
   "source": [
    "### Conclusiones de la Ingeniería de Características\n",
    "\n",
    "El proceso de ingeniería de características implementado ha transformado el conjunto de datos original de calidad del agua subterránea en representaciones optimizadas para el análisis de clustering. Las mejoras clave incluyen:\n",
    "\n",
    "1. **Eliminación de Redundancia**: A través del análisis de correlación, identificamos y eliminamos variables altamente correlacionadas, reduciendo la multicolinealidad que podría distorsionar los resultados del clustering.\n",
    "\n",
    "2. **Generación de Características Informativas**: Creamos características compuestas que capturan aspectos multidimensionales de la calidad del agua, como el índice de calidad ponderado y ratios de balance mineral, incorporando conocimiento de dominio en el proceso.\n",
    "\n",
    "3. **Normalización de Distribuciones**: Aplicamos transformaciones logarítmicas para mitigar el fuerte sesgo observado en el EDA, haciendo que las distribuciones sean más adecuadas para algoritmos basados en distancia.\n",
    "\n",
    "4. **Estandarización de Escalas**: Implementamos la estandarización para asegurar que todas las variables contribuyan equitativamente al análisis, independientemente de sus unidades o rangos originales.\n",
    "\n",
    "5. **Reducción Dimensionalidad Preservando Varianza**: Aplicamos PCA para reducir la dimensionalidad mientras preservamos al menos el 90% de la varianza total, facilitando un clustering más eficiente y robusto.\n",
    "\n",
    "6. **Estrategia Flexible de Representación**: Generamos múltiples conjuntos de datos (completo, PCA e híbrido) que permiten diferentes enfoques de clustering según los objetivos específicos del análisis.\n",
    "\n",
    "El resultado es un proceso de ingeniería de características sistemático, guiado por los hallazgos del EDA y orientado a optimizar la efectividad de los algoritmos de clustering para la segmentación de la calidad del agua subterránea.\n",
    "\n",
    "Los conjuntos de datos resultantes están listos para ser utilizados en diferentes algoritmos de clustering como K-means, DBSCAN o agrupamiento jerárquico, permitiendo identificar patrones significativos en la calidad del agua subterránea de México."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
