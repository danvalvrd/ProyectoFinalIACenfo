{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b1c2d3",
   "metadata": {},
   "source": [
    "# Plan de Transformación de Datos para Modelado de Machine Learning\n",
    "\n",
    "### Estudiantes\n",
    "Amanda Alpízar Araya\n",
    "Alonso Arias Mora\n",
    "Danny Valverde Agüero\n",
    "\n",
    "### Introducción y Objetivos\n",
    "\n",
    "Este notebook detalla el proceso de transformación del conjunto de datos de calidad del agua, previamente limpiado en el notebook `EDA.ipynb`. El objetivo es convertir el DataFrame `df_cleaned` en un conjunto de datos completamente numérico, escalado y optimizado, listo para ser utilizado en algoritmos de machine learning, específicamente para tareas de clustering no supervisado como K-Means.\n",
    "\n",
    "El proceso sigue un plan estructurado para garantizar que las transformaciones se apliquen en un orden lógico, maximizando la calidad de los datos para el modelado.\n",
    "\n",
    "### Metodología de Transformación\n",
    "\n",
    "El pipeline de transformación se ejecutará en el siguiente orden:\n",
    "\n",
    "1.  **Selección de Características (Fase 1):** Se eliminarán columnas de alta cardinalidad, identificadores y variables categóricas redundantes. La columna `CONTAMINANTES` se conservará para ingeniería de características.\n",
    "2.  **Codificación de Variables Categóricas:** Las variables categóricas se convertirán a formato numérico. Se usará **Multi-Label Binarization** para la columna `CONTAMINANTES` y **One-Hot Encoding** para las demás.\n",
    "3.  **Transformación no Lineal:** Se aplicará una transformación logarítmica a las características numéricas para corregir el fuerte sesgo positivo en sus distribuciones.\n",
    "4.  **Selección de Características (Fase 2):** Se analizará la matriz de correlación para identificar y eliminar características altamente correlacionadas, reduciendo la multicolinealidad.\n",
    "5.  **Escalado de Datos:** Todas las características se escalarán utilizando estandarización para asegurar que todas contribuyan por igual al modelo.\n",
    "\n",
    "### Resultado Final\n",
    "\n",
    "El producto de este notebook será un DataFrame final, `df_scaled`, que contendrá únicamente datos numéricos y escalados, representando la versión más óptima del conjunto de datos para la fase de modelado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5d6c7",
   "metadata": {},
   "source": [
    "### Sección 1: Configuración del Entorno y Carga de Datos Limpios\n",
    "\n",
    "#### Resumen Ejecutivo de la Sección\n",
    "\n",
    "Esta sección inicial configura el entorno de Python y carga el conjunto de datos `df_cleaned`, que es el resultado del proceso de limpieza del notebook `EDA.ipynb`. Se asume que este archivo ya ha sido generado y guardado. Realizaremos una inspección inicial para confirmar que los datos se cargan correctamente antes de comenzar con el pipeline de transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sección 1: Configuración del Entorno y Carga de Datos Limpios ---\n",
    "\n",
    "# Importar las librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ignorar advertencias para una salida limpia\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurar el estilo y tamaño de las gráficas\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# --- 1. Cargar el conjunto de datos limpio ---\n",
    "# Este archivo es el resultado del notebook EDA.ipynb\n",
    "cleaned_data_path = 'datos/df_cleaned.csv' \n",
    "try:\n",
    "    # Asumimos que el df_cleaned fue guardado con su índice, por lo que lo usamos como index_col\n",
    "    df_cleaned = pd.read_csv(cleaned_data_path, index_col=0)\n",
    "    print(f\"El conjunto de datos limpio se cargó exitosamente desde: '{cleaned_data_path}'.\")\n",
    "    print(f\"Dimensiones del dataset: {df_cleaned.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró el archivo en la ruta: '{cleaned_data_path}'\")\n",
    "    print(\"Por favor, ejecute primero el notebook EDA.ipynb para generar este archivo.\")\n",
    "    df_cleaned = None\n",
    "\n",
    "# --- 2. Mostrar información inicial del DataFrame ---\n",
    "if df_cleaned is not None:\n",
    "    print(\"\\n--- Primeras 5 filas del conjunto de datos limpio ---\")\n",
    "    display(df_cleaned.head())\n",
    "    print(\"\\n--- Tipos de datos de las columnas ---\")\n",
    "    df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3g4",
   "metadata": {},
   "source": [
    "### Sección 2: Selección de Características (Fase 1)\n",
    "\n",
    "#### Resumen Ejecutivo de la Sección\n",
    "\n",
    "En esta sección, se realiza la primera fase de selección de características. El objetivo es eliminar columnas que no son adecuadas para el modelado, basándose en los hallazgos del EDA. Esto incluye la eliminación de identificadores únicos, variables geográficas de alta cardinalidad y columnas categóricas que son redundantes. **Crucialmente, la columna `CONTAMINANTES` se conserva para ser procesada en la siguiente etapa.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h5i6j7k8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sección 2: Selección de Características (Fase 1) ---\n",
    "\n",
    "if df_cleaned is not None:\n",
    "    # Copiar el dataframe para mantener el original intacto\n",
    "    df_features = df_cleaned.copy()\n",
    "\n",
    "    # --- 1. Identificar columnas a eliminar ---\n",
    "    \n",
    "    # Columnas de identificadores únicos\n",
    "    id_cols = ['CLAVE', 'SITIO']\n",
    "\n",
    "    # Columnas categóricas de alta cardinalidad\n",
    "    high_cardinality_cols = ['MUNICIPIO', 'ACUIFERO']\n",
    "\n",
    "    # Columnas categóricas redundantes (calidad, cumplimiento, semáforo)\n",
    "    # Se excluye 'CONTAMINANTES' de esta lista para conservarla\n",
    "    redundant_categorical_cols = [col for col in df_features.columns if 'CALIDAD_' in col or 'CUMPLE_CON_' in col]\n",
    "    redundant_categorical_cols.append('SEMAFORO')\n",
    "    \n",
    "    # Columna de PERIODO (constante, no aporta varianza)\n",
    "    misc_cols_to_drop = ['PERIODO']\n",
    "\n",
    "    # Combinar todas las columnas a eliminar\n",
    "    cols_to_drop = id_cols + high_cardinality_cols + redundant_categorical_cols + misc_cols_to_drop\n",
    "    # Asegurarse de que no haya duplicados en la lista\n",
    "    cols_to_drop = list(set(cols_to_drop))\n",
    "\n",
    "    # --- 2. Eliminar las columnas ---\n",
    "    \n",
    "    original_col_count = df_features.shape[1]\n",
    "    df_features.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "    new_col_count = df_features.shape[1]\n",
    "\n",
    "    print(f\"--- Selección de Características (Fase 1) ---\")\n",
    "    print(f\"Se eliminaron {original_col_count - new_col_count} columnas.\")\n",
    "    print(f\"El número de columnas pasó de {original_col_count} a {new_col_count}.\")\n",
    "\n",
    "    print(\"\\n--- Columnas restantes en el DataFrame ---\")\n",
    "    display(df_features.head())\n",
    "    df_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l9m0n1o2",
   "metadata": {},
   "source": [
    "### Sección 3: Codificación de Variables Categóricas\n",
    "\n",
    "#### Resumen Ejecutivo de la Sección\n",
    "\n",
    "Esta sección convierte todas las variables categóricas en formato numérico. Se aplica un enfoque de dos pasos: primero, se utiliza **Multi-Label Binarization** en la columna `CONTAMINANTES` para crear una característica binaria para cada tipo de contaminante. Segundo, se aplica **One-Hot Encoding** a las demás variables categóricas de baja cardinalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p3q4r5s6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sección 3: Codificación de Variables Categóricas ---\n",
    "\n",
    "if 'df_features' in locals():\n",
    "    df_encoded = df_features.copy()\n",
    "    \n",
    "    # --- 1. Codificación Multi-Label para 'CONTAMINANTES' ---\n",
    "    print(\"--- Codificación Multi-Label para 'CONTAMINANTES' ---\")\n",
    "    \n",
    "    # Limpiar el texto y dividirlo en una lista de contaminantes\n",
    "    # Se reemplaza el valor de no-contaminación para que no se divida\n",
    "    df_encoded['CONTAMINANTES'] = df_encoded['CONTAMINANTES'].str.replace('Sin Contaminantes', 'NINGUNO')\n",
    "    \n",
    "    # Crear variables dummy a partir de las etiquetas separadas por coma\n",
    "    contaminant_dummies = df_encoded['CONTAMINANTES'].str.get_dummies(sep=', ')\n",
    "    \n",
    "    # Renombrar columnas para claridad\n",
    "    contaminant_dummies.columns = [f'CONT_{col.replace(\" \", \"_\")}' for col in contaminant_dummies.columns]\n",
    "    print(f\"Se crearon {contaminant_dummies.shape[1]} nuevas características desde 'CONTAMINANTES'.\")\n",
    "\n",
    "    # Unir las nuevas columnas y eliminar la original\n",
    "    df_encoded = pd.concat([df_encoded, contaminant_dummies], axis=1)\n",
    "    df_encoded.drop('CONTAMINANTES', axis=1, inplace=True)\n",
    "    \n",
    "    # --- 2. One-Hot Encoding para el resto de categóricas ---\n",
    "    categorical_cols = df_encoded.select_dtypes(include=['object', 'category']).columns\n",
    "    if not categorical_cols.empty:\n",
    "        print(f\"\\n--- Codificación One-Hot para {len(categorical_cols)} variables restantes ---\")\n",
    "        print(\"Columnas a codificar:\", list(categorical_cols))\n",
    "        df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=True)\n",
    "    else:\n",
    "        print(\"\\nNo hay otras columnas categóricas para codificar.\")\n",
    "\n",
    "    print(f\"\\nEl número total de columnas es ahora {df_encoded.shape[1]}.\")\n",
    "    print(\"\\n--- DataFrame después de la Codificación Completa ---\")\n",
    "    display(df_encoded.head())\n",
    "    df_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t7u8v9w0",
   "metadata": {},
   "source": [
    "### Sección 4: Transformación no Lineal (Logarítmica)\n",
    "\n",
    "#### Resumen Ejecutivo de la Sección\n",
    "\n",
    "En esta sección, se aborda el fuerte sesgo positivo detectado en las distribuciones de las variables de medición. Se aplicará una transformación logarítmica (`log(1+x)`) para normalizar estas distribuciones. Esta transformación comprime el rango de los valores grandes y expande el de los valores pequeños, haciendo que la distribución sea más simétrica. Esto es crucial para el rendimiento de algoritmos sensibles a la escala y la distribución, como K-Means y PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x1y2z3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sección 4: Transformación no Lineal (Logarítmica) ---\n",
    "\n",
    "if 'df_encoded' in locals():\n",
    "    # Identificar las columnas de mediciones originales (excluyendo coordenadas y columnas ya codificadas)\n",
    "    measurement_cols = df_cleaned.select_dtypes(include=np.number).columns\n",
    "    # Excluir latitud y longitud, ya que no suelen requerir esta transformación\n",
    "    cols_to_transform = [col for col in measurement_cols if col in df_encoded.columns and col not in ['LATITUD', 'LONGITUD']]\n",
    "    \n",
    "    print(f\"--- Aplicando Transformación Logarítmica a {len(cols_to_transform)} columnas ---\")\n",
    "    \n",
    "    # Crear una copia para la transformación\n",
    "    df_transformed = df_encoded.copy()\n",
    "\n",
    "    # Visualización Antes y Después para una columna de ejemplo\n",
    "    example_col = 'CONDUCT_mS/cm'\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df_transformed[example_col], kde=True)\n",
    "    plt.title(f'Distribución de {example_col} (Antes de la Transformación)')\n",
    "\n",
    "    # Aplicar la transformación log1p a todas las columnas seleccionadas\n",
    "    for col in cols_to_transform:\n",
    "        df_transformed[col] = np.log1p(df_transformed[col])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(df_transformed[example_col], kde=True, color='green')\n",
    "    plt.title(f'Distribución de {example_col} (Después de la Transformación Log)')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTransformación logarítmica completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6d7e8",
   "metadata": {},
   "source": [
    "### Sección 5: Selección de Características (Fase 2 - Correlación)\n",
    "\n",
    "#### Resumen Ejecutivo de la Sección\n",
    "\n",
    "Ahora que los datos son numéricos y sus distribuciones han sido mejoradas, se procede a la segunda fase de selección de características. El objetivo es identificar y eliminar la multicolinealidad, que ocurre cuando dos o más características están altamente correlacionadas. Se calculará una matriz de correlación y se eliminará una de cada par de características cuya correlación supere un umbral predefinido (e.g., 0.90), conservando así la que aparezca primero en el DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9g0h1i2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sección 5: Selección de Características (Fase 2 - Correlación) ---\n",
    "\n",
    "if 'df_transformed' in locals():\n",
    "    # Calcular la matriz de correlación\n",
    "    corr_matrix = df_transformed.corr().abs()\n",
    "\n",
    "    # Visualizar el heatmap de correlación\n",
    "    plt.figure(figsize=(24, 20))\n",
    "    sns.heatmap(corr_matrix, cmap='viridis', annot=False)\n",
    "    plt.title('Heatmap de Correlación de Características Transformadas')\n",
    "    plt.show()\n",
    "\n",
    "    # Identificar y eliminar características altamente correlacionadas\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column] > 0.90)]\n",
    "\n",
    "    print(f\"--- Análisis de Correlación (Umbral > 0.90) ---\")\n",
    "    if to_drop_corr:\n",
    "        print(f\"Se encontraron {len(to_drop_corr)} columnas para eliminar debido a alta correlación:\")\n",
    "        print(to_drop_corr)\n",
    "        df_reduced = df_transformed.drop(columns=to_drop_corr)\n",
    "        print(f\"\\nEl número de columnas se redujo de {df_transformed.shape[1]} a {df_reduced.shape[1]}.\")\n",
    "    else:\n",
    "        print(\"No se encontraron características con una correlación superior al umbral.\")\n",
    "        df_reduced = df_transformed.copy()\n",
    "\n",
    "    print(\"\\n--- DataFrame después de eliminar alta correlación ---\")\n",
    "    display(df_reduced.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j3k4l5m6",
   "metadata": {},
   "source": [
    "### Sección 6: Escalado de Datos (Estandarización)\n",
    "\n",
    "#### Resumen Ejecutivo de la Sección\n",
    "\n",
    "Este es el paso final de la preparación de datos. Las características, aunque transformadas, todavía se encuentran en diferentes escalas. Para asegurar que los algoritmos de clustering (como K-Means) traten a todas las características con la misma importancia, se aplica la estandarización. Este proceso reescala cada característica para que tenga una media de 0 y una desviación estándar de 1. El resultado es el DataFrame final, `df_scaled`, listo para el modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n7o8p9q0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sección 6: Escalado de Datos (Estandarización) ---\n",
    "\n",
    "if 'df_reduced' in locals():\n",
    "    # Inicializar el escalador\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Aplicar el escalador\n",
    "    scaled_features = scaler.fit_transform(df_reduced)\n",
    "\n",
    "    # Convertir el resultado de nuevo a un DataFrame\n",
    "    df_scaled = pd.DataFrame(scaled_features, index=df_reduced.index, columns=df_reduced.columns)\n",
    "\n",
    "    print(\"--- Escalado de Datos Completado ---\")\n",
    "    print(\"Todas las características han sido estandarizadas (media=0, std=1).\")\n",
    "    \n",
    "    print(\"\\n--- Vista Previa del DataFrame Final Escalado ---\")\n",
    "    display(df_scaled.head())\n",
    "\n",
    "    print(\"\\n--- Verificación de la media y desviación estándar después del escalado ---\")\n",
    "    display(df_scaled.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r1s2t3u4",
   "metadata": {},
   "source": [
    "### Sección 7: Resumen y Conclusiones Finales\n",
    "\n",
    "#### Resumen de Transformaciones\n",
    "\n",
    "El conjunto de datos ha sido sometido a un riguroso pipeline de transformaciones para prepararlo para el modelado de machine learning. Las operaciones realizadas fueron:\n",
    "\n",
    "1.  **Selección de Características (Fase 1):** Se eliminaron columnas de identificadores, alta cardinalidad y características derivadas redundantes, conservando `CONTAMINANTES`.\n",
    "2.  **Codificación:** La columna `CONTAMINANTES` se expandió a 17 características binarias. Las variables `ORGANISMO_DE_CUENCA` y `SUBTIPO` se codificaron en 14 columnas. \n",
    "3.  **Transformación Logarítmica:** Se aplicó `np.log1p` a 14 variables de medición para normalizar sus distribuciones.\n",
    "4.  **Selección de Características (Fase 2):** Se eliminó 1 columna (`SDT_M_mg/L`) debido a su alta correlación (>0.90) con `CONDUCT_mS/cm`.\n",
    "5.  **Estandarización:** Todas las características finales fueron escaladas a una media de 0 y una desviación estándar de 1.\n",
    "\n",
    "#### Conclusión\n",
    "\n",
    "El resultado final es el DataFrame `df_scaled`. Este conjunto de datos es numérico, no tiene valores faltantes, presenta distribuciones mejoradas, está libre de multicolinealidad severa y todas sus características están en la misma escala.\n",
    "\n",
    "El conjunto de datos está ahora en condiciones óptimas para ser utilizado en algoritmos de clustering como K-Means o para análisis de componentes principales (PCA), que sería un siguiente paso lógico para explorar la estructura latente de los datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
