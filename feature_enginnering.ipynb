{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Water Quality Clustering\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook details the feature engineering pipeline designed to prepare the cleaned water quality dataset for unsupervised clustering. The primary objective is to transform the raw variables into a production-ready, numerical feature set optimized for a K-Means model.\n",
    "\n",
    "The strategy is guided by the findings from the `EDA.ipynb` notebook, which highlighted several key data characteristics:\n",
    "1.  **High Cardinality:** Categorical location features like `MUNICIPIO` and `ACUIFERO` are too complex for direct use.\n",
    "2.  **Redundancy:** The dataset contains redundant information, such as ID columns, constant columns, and pairs of numerical measurements with their derived quality labels.\n",
    "3.  **Distribution Skew:** Many numerical measurements exhibit a strong right-skew, which can negatively impact distance-based algorithms like K-Means.\n",
    "4.  **Multicollinearity:** High correlation was noted between variables related to salinity and dissolved solids.\n",
    "\n",
    "The following steps will systematically address these issues to produce a final, scaled, and robust feature set for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initial Feature Selection (Exclusion of Uninformative Columns)\n",
    "\n",
    "**Justification:** The first step is to remove columns that provide no value for a clustering model or introduce noise. This includes identifiers, constants, high-cardinality categoricals, and redundant derived labels.\n",
    "\n",
    "1.  **Identifier Columns (`CLAVE`, `SITIO`):** These are unique IDs for each monitoring site and have no predictive power. Their high cardinality (1068 and 1066 unique values, respectively) would add noise.\n",
    "2.  **Constant Column (`PERIODO`):** This column has only one value (2020) and thus offers no variance or information to the model.\n",
    "3.  **High-Cardinality Categorical Columns (`ORGANISMO_DE_CUENCA`, `ESTADO`, `MUNICIPIO`, `ACUIFERO`, `SUBTIPO`):** As identified in the EDA, `MUNICIPIO` (452 unique values) and `ACUIFERO` (273 unique values) are too complex to be one-hot encoded. The geographic information is better and more efficiently represented by the `LONGITUD` and `LATITUD` columns.\n",
    "4.  **Redundant Categorical/Compliance Columns (`CALIDAD_*`, `CUMPLE_CON_*`, `SEMAFORO`):** These columns are derived directly from the numerical measurements (e.g., `CALIDAD_AS` is derived from `AS_TOT_mg/L`). Including both the raw measurement and its categorical label is redundant. We will retain the numerical columns as they contain more granular information, which is ideal for clustering.\n",
    "5.  **Multi-Label Text Column (`CONTAMINANTES`):** This column summarizes which numerical features exceed a certain threshold. It is redundant information already captured by the numerical columns themselves and will be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This cell assumes 'df_cleaned' is the pre-processed DataFrame from the EDA notebook\n",
    "# In a real pipeline, you would load it: df_cleaned = pd.read_csv('path/to/cleaned_data.csv')\n",
    "\n",
    "def initial_feature_selection(df):\n",
    "    \"\"\"Removes uninformative, redundant, and high-cardinality columns.\"\"\"\n",
    "    # Identify all 'CALIDAD_*' and 'CUMPLE_CON_*' columns dynamically\n",
    "    quality_cols = [col for col in df.columns if col.startswith('CALIDAD_')]\n",
    "    compliance_cols = [col for col in df.columns if col.startswith('CUMPLE_CON_')]\n",
    "\n",
    "    cols_to_drop = [\n",
    "        # 1. Identifiers\n",
    "        'CLAVE', 'SITIO',\n",
    "        # 2. Constants\n",
    "        'PERIODO',\n",
    "        # 3. High-Cardinality Categoricals\n",
    "        'ORGANISMO_DE_CUENCA', 'ESTADO', 'MUNICIPIO', 'ACUIFERO', 'SUBTIPO',\n",
    "        # 4. Redundant Categorical/Compliance Columns\n",
    "        'SEMAFORO', \n",
    "        *quality_cols,\n",
    "        *compliance_cols,\n",
    "        # 5. Redundant Multi-Label Text\n",
    "        'CONTAMINANTES'\n",
    "    ]\n",
    "    \n",
    "    # Ensure columns exist before dropping to prevent errors\n",
    "    cols_to_drop_existing = [col for col in cols_to_drop if col in df.columns]\n",
    "    \n",
    "    df_selected = df.drop(columns=cols_to_drop_existing)\n",
    "    \n",
    "    print(\"Columns dropped:\", len(cols_to_drop_existing))\n",
    "    print(\"Remaining columns:\", df_selected.shape[1])\n",
    "    print(\"--- Final Selected Features ---\")\n",
    "    print(df_selected.columns.tolist())\n",
    "    \n",
    "    return df_selected\n",
    "\n",
    "# Example usage (assuming df_cleaned is available)\n",
    "# features_selected = initial_feature_selection(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Logarithmic Transformation of Skewed Features\n",
    "\n",
    "**Justification:** The EDA revealed that most numerical water quality measurements are heavily right-skewed. Distance-based algorithms like K-Means are sensitive to skewed distributions and outliers, as they can dominate the distance calculations. Applying a logarithmic transformation (`log1p`, which handles zero values) compresses the range of these variables, reduces the impact of extreme outliers, and makes the distributions more symmetric, leading to more stable and meaningful clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_log_transformation(df):\n",
    "    \"\"\"Applies a log1p transformation to all numerical columns except lat/long.\"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    # Exclude geographic coordinates from the transformation\n",
    "    cols_to_transform = [col for col in df_transformed.columns if col not in ['LONGITUD', 'LATITUD']]\n",
    "    \n",
    "    for col in cols_to_transform:\n",
    "        df_transformed[col] = np.log1p(df_transformed[col])\n",
    "        \n",
    "    print(f\"Log1p transformation applied to {len(cols_to_transform)} columns.\")\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "# Example usage:\n",
    "# features_log_transformed = apply_log_transformation(features_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Scaling Features\n",
    "\n",
    "**Justification:** The final step before modeling is to scale the features. K-Means calculates distances between data points, so variables with larger scales and variances (e.g., `CONDUCT_mS/cm`) would have a disproportionately large influence on the cluster assignments compared to variables with smaller scales (e.g., `AS_TOT_mg/L`). Using `StandardScaler` standardizes each feature by subtracting the mean and dividing by the standard deviation. This ensures that all features have a mean of 0 and a standard deviation of 1, giving them equal weight in the clustering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_features(df):\n",
    "    \"\"\"Scales all features using StandardScaler.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # The scaler returns a numpy array, so we convert it back to a DataFrame\n",
    "    scaled_features_array = scaler.fit_transform(df)\n",
    "    df_scaled = pd.DataFrame(scaled_features_array, columns=df.columns, index=df.index)\n",
    "    \n",
    "    print(\"All features have been scaled using StandardScaler.\")\n",
    "    print(\"--- Final Production-Ready Features ---\")\n",
    "    display(df_scaled.head())\n",
    "    \n",
    "    return df_scaled, scaler\n",
    "\n",
    "# Example usage:\n",
    "# final_features, scaler_object = scale_features(features_log_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Pipeline Execution\n",
    "\n",
    "This cell combines all the above steps into a single, executable pipeline to transform the cleaned data into the final, model-ready feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_pipeline(df_cleaned):\n",
    "    \"\"\"Executes the full feature engineering pipeline.\"\"\"\n",
    "    print(\"--- Starting Feature Engineering Pipeline ---\\n\")\n",
    "    \n",
    "    # Step 1: Initial Feature Selection\n",
    "    print(\"Step 1: Performing initial feature selection...\")\n",
    "    features_selected = initial_feature_selection(df_cleaned)\n",
    "    print(\"\\n--------------------------------------------\\n\")\n",
    "    \n",
    "    # Step 2: Logarithmic Transformation\n",
    "    print(\"Step 2: Applying logarithmic transformation...\")\n",
    "    features_log_transformed = apply_log_transformation(features_selected)\n",
    "    print(\"\\n--------------------------------------------\\n\")\n",
    "    \n",
    "    # Step 3: Scaling Features\n",
    "    print(\"Step 3: Scaling features...\")\n",
    "    final_features, scaler = scale_features(features_log_transformed)\n",
    "    print(\"\\n--- Feature Engineering Pipeline Complete ---\")\n",
    "    \n",
    "    return final_features, scaler\n",
    "\n",
    "# To run the pipeline, you would need the 'df_cleaned' DataFrame from the EDA.\n",
    "# For example:\n",
    "# df_cleaned = pd.read_csv('data/cleaned_water_quality_2020.csv')\n",
    "# final_model_features, production_scaler = create_feature_pipeline(df_cleaned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
