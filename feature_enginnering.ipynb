{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Water Quality Clustering\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook details the feature engineering pipeline designed to prepare the cleaned water quality dataset for unsupervised clustering. The primary objective is to transform the raw variables into a production-ready, numerical feature set optimized for a K-Means model.\n",
    "\n",
    "The strategy is guided by the findings from the `EDA.ipynb` notebook, which highlighted several key data characteristics:\n",
    "1.  **High Cardinality:** Categorical location features like `MUNICIPIO` and `ACUIFERO` are too complex for direct use.\n",
    "2.  **Redundancy:** The dataset contains redundant information, such as ID columns, constant columns, and pairs of numerical measurements with their derived quality labels.\n",
    "3.  **Distribution Skew:** Many numerical measurements exhibit a strong right-skew, which can negatively impact distance-based algorithms like K-Means.\n",
    "4.  **Multicollinearity:** High correlation was noted between variables related to salinity and dissolved solids.\n",
    "\n",
    "The following steps will systematically address these issues to produce a final, scaled, and robust feature set for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initial Feature Selection (Exclusion of Uninformative Columns)\n",
    "\n",
    "**Justification:** The first step is to remove columns that provide no value for a clustering model or introduce noise. This includes identifiers, constants, high-cardinality categoricals, and redundant derived labels.\n",
    "\n",
    "1.  **Identifier Columns (`CLAVE`, `SITIO`):** These are unique IDs for each monitoring site and have no predictive power. Their high cardinality (1068 and 1066 unique values, respectively) would add noise.\n",
    "2.  **Constant Column (`PERIODO`):** This column has only one value (2020) and thus offers no variance or information to the model.\n",
    "3.  **High-Cardinality Categorical Columns (`ORGANISMO_DE_CUENCA`, `ESTADO`, `MUNICIPIO`, `ACUIFERO`, `SUBTIPO`):** As identified in the EDA, `MUNICIPIO` (452 unique values) and `ACUIFERO` (273 unique values) are too complex to be one-hot encoded. The geographic information is better and more efficiently represented by the `LONGITUD` and `LATITUD` columns.\n",
    "4.  **Redundant Categorical/Compliance Columns (`CALIDAD_*`, `CUMPLE_CON_*`, `SEMAFORO`):** These columns are derived directly from the numerical measurements (e.g., `CALIDAD_AS` is derived from `AS_TOT_mg/L`). Including both the raw measurement and its categorical label is redundant. We will retain the numerical columns as they contain more granular information, which is ideal for clustering.\n",
    "5.  **Multi-Label Text Column (`CONTAMINANTES`):** This column summarizes which numerical features exceed a certain threshold. It is redundant information already captured by the numerical columns themselves and will be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This cell assumes 'df_cleaned' is the pre-processed DataFrame from the EDA notebook\n",
    "# In a real pipeline, you would load it: df_cleaned = pd.read_csv('path/to/cleaned_data.csv')\n",
    "\n",
    "def initial_feature_selection(df):\n",
    "    \"\"\"Removes uninformative, redundant, and high-cardinality columns.\"\"\"\"\n",
    "    # Identify all 'CALIDAD_*' and 'CUMPLE_CON_*' columns dynamically\n",
    "    quality_cols = [col for col in df.columns if col.startswith('CALIDAD_')]\n",
    "    compliance_cols = [col for col in df.columns if col.startswith('CUMPLE_CON_')]\n",
    "\n",
    "    cols_to_drop = [\n",
    "        # 1. Identifiers\n",
    "        'CLAVE', 'SITIO',\n",
    "        # 2. Constants\n",
    "        'PERIODO',\n",
    "        # 3. High-Cardinality Categoricals\n",
    "        'ORGANISMO_DE_CUENCA', 'ESTADO', 'MUNICIPIO', 'ACUIFERO', 'SUBTIPO',\n",
    "        # 4. Redundant Categorical/Compliance Columns\n",
    "        'SEMAFORO', \n",
    "        *quality_cols,\n",
    "        *compliance_cols,\n",
    "        # 5. Redundant Multi-Label Text\n",
    "        'CONTAMINANTES'\n",
    "    ]\n",
    "    \n",
    "    # Ensure columns exist before dropping to prevent errors\n",
    "    cols_to_drop_existing = [col for col in cols_to_drop if col in df.columns]\n",
    "    \n",
    "    df_selected = df.drop(columns=cols_to_drop_existing)\n",
    "    \n",
    "    print(\"Columns dropped:\", len(cols_to_drop_existing))\n",
    "    print(\"Remaining columns:\", df_selected.shape[1])\n",
    "    print(\"--- Final Selected Features ---\")\n",
    "    print(df_selected.columns.tolist())\n",
    "    \n",
    "    return df_selected\n",
    "\n",
    "# Example usage (assuming df_cleaned is available)\n",
    "# features_selected = initial_feature_selection(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Logarithmic Transformation of Skewed Features\n",
    "\n",
    "**Justification:** The EDA revealed that most numerical water quality measurements are heavily right-skewed. Distance-based algorithms like K-Means are sensitive to skewed distributions and outliers, as they can dominate the distance calculations. Applying a logarithmic transformation (`log1p`, which handles zero values) compresses the range of these variables, reduces the impact of extreme outliers, and makes the distributions more symmetric, leading to more stable and meaningful clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_log_transformation(df):\n",
    "    \"\"\"Applies a log1p transformation to all numerical columns except lat/long.\"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    # Exclude geographic coordinates from the transformation\n",
    "    cols_to_transform = [col for col in df_transformed.columns if col not in ['LONGITUD', 'LATITUD']]\n",
    "    \n",
    "    for col in cols_to_transform:\n",
    "        df_transformed[col] = np.log1p(df_transformed[col])\n",
    "        \n",
    "    print(f\"Log1p transformation applied to {len(cols_to_transform)} columns.\")\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "# Example usage:\n",
    "# features_log_transformed = apply_log_transformation(features_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Scaling Features\n",
    "\n",
    "**Justification:** The final step before modeling is to scale the features. K-Means calculates distances between data points, so variables with larger scales and variances (e.g., `CONDUCT_mS/cm`) would have a disproportionately large influence on the cluster assignments compared to variables with smaller scales (e.g., `AS_TOT_mg/L`). Using `StandardScaler` standardizes each feature by subtracting the mean and dividing by the standard deviation. This ensures that all features have a mean of 0 and a standard deviation of 1, giving them equal weight in the clustering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_features(df):\n",
    "    \"\"\"Scales all features using StandardScaler.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # The scaler returns a numpy array, so we convert it back to a DataFrame\n",
    "    scaled_features_array = scaler.fit_transform(df)\n",
    "    df_scaled = pd.DataFrame(scaled_features_array, columns=df.columns, index=df.index)\n",
    "    \n",
    "    print(\"All features have been scaled using StandardScaler.\")\n",
    "    print(\"--- Final Production-Ready Features ---\")\n",
    "    display(df_scaled.head())\n",
    "    \n",
    "    return df_scaled, scaler\n",
    "\n",
    "# Example usage:\n",
    "# final_features, scaler_object = scale_features(features_log_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Discretization (Binning) of Numerical Features\n",
    "\n",
    "**Justification:** As an alternative to direct scaling and transformation, we can bin numerical features. This approach converts continuous variables into categorical ones, which can be beneficial for several reasons:\n",
    "1.  **Handles Outliers:** Extreme values are grouped into the highest or lowest bin, reducing their skewing effect.\n",
    "2.  **Captures Non-Linearity:** It allows models (especially linear ones) to learn non-linear patterns in the data.\n",
    "3.  **Improves Robustness:** The model becomes less sensitive to small fluctuations in the input data.\n",
    "\n",
    "We will use quantile-based binning (`pd.qcut`), which divides the data into bins with an equal number of observations, making it robust to skewed distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_binning(df, n_bins=5):\n",
    "    \"\"\"Applies quantile-based binning to numerical columns and returns a one-hot encoded DataFrame.\"\"\"\n",
    "    df_binned = df.copy()\n",
    "    \n",
    "    # Identify numerical columns to bin, excluding geographic coordinates\n",
    "    cols_to_bin = [col for col in df_binned.columns if col not in ['LONGITUD', 'LATITUD']]\n",
    "    \n",
    "    binned_dfs = []\n",
    "    \n",
    "    for col in cols_to_bin:\n",
    "        # Create bins and one-hot encode them\n",
    "        binned_col = pd.qcut(df_binned[col], q=n_bins, labels=False, duplicates='drop')\n",
    "        one_hot_binned = pd.get_dummies(binned_col, prefix=f\"{col}_bin\")\n",
    "        binned_dfs.append(one_hot_binned)\n",
    "    \n",
    "    # Combine the one-hot encoded bins with the original lat/long\n",
    "    df_final_binned = pd.concat([df_binned[['LONGITUD', 'LATITUD']]] + binned_dfs, axis=1)\n",
    "    \n",
    "    print(f\"Applied {n_bins}-quantile binning to {len(cols_to_bin)} columns.\")\n",
    "    print(f\"Resulting feature space has {df_final_binned.shape[1]} columns after one-hot encoding.\")\n",
    "    \n",
    "    return df_final_binned\n",
    "\n",
    "# Example Usage:\n",
    "# features_binned = apply_binning(features_selected, n_bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Pipeline Execution\n",
    "\n",
    "This cell combines all the above steps into a single, executable pipeline to transform the cleaned data into the final, model-ready feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_pipeline(df_cleaned, use_binning=False):\n",
    "    \"\"\"Executes the full feature engineering pipeline.\"\"\"\n",
    "    print(\"--- Starting Feature Engineering Pipeline ---\\n\")\n",
    "    \n",
    "    # Step 1: Initial Feature Selection (common to both paths)\n",
    "    print(\"Step 1: Performing initial feature selection...\")\n",
    "    features_selected = initial_feature_selection(df_cleaned)\n",
    "    print(\"\\n--------------------------------------------\\n\")\n",
    "    \n",
    "    if use_binning:\n",
    "        # Path 2: Binning and One-Hot Encoding\n",
    "        print(\"Step 2 (Alternative): Applying quantile-based binning...\")\n",
    "        final_features = apply_binning(features_selected, n_bins=5)\n",
    "        scaler = None # No scaler needed for this path\n",
    "    else:\n",
    "        # Path 1: Log Transformation and Scaling (for distance-based models)\n",
    "        print(\"Step 2: Applying logarithmic transformation...\")\n",
    "        features_log_transformed = apply_log_transformation(features_selected)\n",
    "        print(\"\\n--------------------------------------------\\n\")\n",
    "        \n",
    "        print(\"Step 3: Scaling features...\")\n",
    "        final_features, scaler = scale_features(features_log_transformed)\n",
    "\n",
    "    print(\"\\n--- Feature Engineering Pipeline Complete ---\")\n",
    "    \n",
    "    return final_features, scaler\n",
    "\n",
    "# To run the pipeline, you would need the 'df_cleaned' DataFrame from the EDA.\n",
    "# For example:\n",
    "# df_cleaned = pd.read_csv('data/cleaned_water_quality_2020.csv')\n",
    "\n",
    "# To get log-transformed and scaled features (for K-Means):\n",
    "# final_kmeans_features, production_scaler = create_feature_pipeline(df_cleaned, use_binning=False)\n",
    "\n",
    "# To get binned and one-hot encoded features (for other models):\n",
    "# final_binned_features, _ = create_feature_pipeline(df_cleaned, use_binning=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
